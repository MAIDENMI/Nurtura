{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568bb680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa98f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_image_click(image):\n",
    "    \"\"\"\n",
    "    Interactive function to click on an image and store coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    image: numpy array of shape (H, W, 3)\n",
    "        The image to display for interaction.\n",
    "\n",
    "    Returns:\n",
    "    clicked_points: list of tuples\n",
    "        A list of coordinates (x, y) of points clicked on the image.\n",
    "    \"\"\"\n",
    "    clicked_points = []\n",
    "    \n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        c, h, w = image.shape\n",
    "        \n",
    "        if c == 3:\n",
    "            image = image.permute(1,2,0)\n",
    "\n",
    "    def onclick(event):\n",
    "        # Check if the click is within the axes\n",
    "        if event.inaxes:\n",
    "            x, y = int(event.xdata), int(event.ydata)\n",
    "            clicked_points.append((x, y))\n",
    "            print(f\"{x}\\t {y}\")\n",
    "    \n",
    "    # Function to handle interaction\n",
    "    def collect_points():\n",
    "        nonlocal clicked_points\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Click on points. Close the window to finish.\")\n",
    "        plt.connect('button_press_event', onclick)\n",
    "        plt.show(block=True)\n",
    "\n",
    "    # Interaction loop\n",
    "    more_points = True\n",
    "    while more_points:\n",
    "        collect_points()\n",
    "        response = input(\"Do you want to select more points? (yes/no): \").strip().lower()\n",
    "        more_points = response in [\"yes\", \"y\"]\n",
    "    \n",
    "    # print(\"Clicked points:\", clicked_points)\n",
    "    return clicked_points\n",
    "\n",
    "\n",
    "def crop_blocks(image, points, block_size=20):\n",
    "    \"\"\"\n",
    "    Crop square blocks centered around given points from the image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image, either of shape (batch, 3, H, W) or (batch, H, W).\n",
    "        points (list): List of (x, y) coordinates for cropping centers.\n",
    "        block_size (int): Size of the square block (default: 15x15).\n",
    "    Returns:\n",
    "        list: List of cropped blocks as numpy arrays.\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 4:\n",
    "        # Case: Image shape is (batch, 3, H, W)\n",
    "        batch, channels, h, w = image.shape\n",
    "        color_image = True\n",
    "    elif len(image.shape) == 3:\n",
    "        # Case: Image shape is (batch, H, W)\n",
    "        batch, h, w = image.shape\n",
    "        channels = None\n",
    "        color_image = False\n",
    "    else:\n",
    "        raise ValueError(\"Image shape must be either (batch, 3, H, W) or (batch, H, W).\")\n",
    "\n",
    "    half_block = block_size // 2\n",
    "    cropped_blocks = []\n",
    "\n",
    " \n",
    "    for (x, y) in points:\n",
    "        # Ensure the cropping stays within image boundaries\n",
    "        y_start = max(y - half_block, 0)\n",
    "        y_end = min(y + half_block + 1, h)\n",
    "        x_start = max(x - half_block, 0)\n",
    "        x_end = min(x + half_block + 1, w)\n",
    "\n",
    "        if color_image:\n",
    "            cropped_block = image[:, :, y_start:y_end, x_start:x_end]  # Handle (3, H, W)\n",
    "        else:\n",
    "            cropped_block = image[:, y_start:y_end, x_start:x_end]  # Handle (H, W)\n",
    "\n",
    "        cropped_blocks.append(cropped_block)\n",
    "\n",
    "    return cropped_blocks\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "time_s = 1\n",
    "space_s = 2 # width of the spatial window\n",
    "time_series_data = {}  # create only once\n",
    "\n",
    "## variable from mtlnet_train_rppg.py\n",
    "# frames = torch.from_numpy(data); frames = frames.permute(0,3,1,2)[:,:,:,:]\n",
    "# frames from MTLnet_train_rppg\n",
    "\n",
    "def plot_cos_or_dot(cosine_sim, clicked_points):\n",
    "\n",
    "    lines_ = []\n",
    "    if clicked_points is None:\n",
    "        clicked_points = interactive_image_click(cosine_sim[200,1])\n",
    "    \n",
    "    cropped_blocks = crop_blocks(cosine_sim, clicked_points, int(space_s))\n",
    "\n",
    "    for line in cropped_blocks:\n",
    "        lines = line.mean(-1).mean(-1)\n",
    "        plt.plot(lines- lines.mean( 0))\n",
    "        \n",
    "        lines_.append(lines-lines.mean(0))\n",
    "    \n",
    "    plt.legend([i for i in range(len(clicked_points))])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return lines_, clicked_points\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Assuming output is a PyTorch tensor, extract the signal\n",
    "\n",
    "# Define the bandpass filter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency is half of the sampling rate\n",
    "    low = lowcut / nyquist  # Normalize the lowcut frequency\n",
    "    high = highcut / nyquist  # Normalize the highcut frequency\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def apply_bandpass_filter(data, lowcut, highcut, fs, order=10):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)  # Apply filter to data\n",
    "    return y\n",
    "\n",
    "def rank_sig(sig_):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    sig_ : a list containing time series signal\n",
    "        DESCRIPTION.\n",
    "    Returns\n",
    "    -------\n",
    "    int: ordered points' index based on their location.\n",
    "\n",
    "    '''\n",
    "    ranked_max = (np.array(sig_)**2).sum(axis = 1)\n",
    "    return ranked_max.argsort()[::-1]\n",
    "    \n",
    "\n",
    "def filt_sig(signal, lowcut = 0.8,  highcut = 4, ch = 1, fs = 10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : List\n",
    "        contain time series signal\n",
    "    lowcut = lower cut off freq\n",
    "    highcut = Higher cut off freq\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sig_ : TYPE\n",
    "        Filtered signal \n",
    "    '''\n",
    "    sig_ = []\n",
    "    for sig in signal:\n",
    "        temp = sig[:,ch]\n",
    "        f_signal = apply_bandpass_filter(temp.T, lowcut, highcut, fs =fs, order= 5)\n",
    "        sig_.append(f_signal)\n",
    "    return sig_, rank_sig(sig_)\n",
    "\n",
    "def save_filtered_graphs(time_series_BR, video_name, i):\n",
    "    \"\"\"\n",
    "    Saves filtered signals for each channel (B, G, R) for 9 points.\n",
    "    - Graphs 1–9: Each shows B, G, and R signals for one point.\n",
    "    - Graph 10: Shows the averaged BGR signal for each point.\n",
    "    File names include descriptive labels: Abdomen, Chest, Shoulder, Control.\n",
    "    \"\"\"\n",
    "\n",
    "    folder_name = video_name\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # ROI labels in the order your points are clicked\n",
    "    roi_labels = [\n",
    "        \"Abdomen Point 1\",\n",
    "        \"Abdomen Point 2\",\n",
    "        \"Chest Point 1\",\n",
    "        \"Chest Point 2\",\n",
    "        \"Shoulder Point 1\",\n",
    "        \"Shoulder Point 2\",\n",
    "        \"Control Point 1\",\n",
    "        \"Control Point 2\",\n",
    "        \"Control Point 3\"\n",
    "    ]\n",
    "\n",
    "    # Extract signals for all channels\n",
    "    channel_keys = [(0, (1, 2)), (1, (1, 2)), (2, (1, 2))]  # B, G, R\n",
    "    signals_by_channel = [time_series_BR[key] for key in channel_keys]\n",
    "\n",
    "    # --- Graphs 1–9: Each point with B, G, R ---\n",
    "    for point_idx, roi_name in enumerate(roi_labels):\n",
    "        plt.figure()\n",
    "        for ch_idx, ch_name in zip(range(3), [\"B\", \"G\", \"R\"]):\n",
    "            plt.plot(signals_by_channel[ch_idx][point_idx], label=ch_name)\n",
    "        plt.title(f\"Filtered Signals - {roi_name}\")\n",
    "        plt.xlabel(\"Frame\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        filename = f\"{roi_name.replace(' ', '_')}_Filtered.png\"\n",
    "        plt.savefig(os.path.join(folder_name, filename))\n",
    "        plt.close()\n",
    "\n",
    "    # --- Graph 10: Averaged BGR signals for all 9 points ---\n",
    "    plt.figure()\n",
    "    for point_idx, roi_name in enumerate(roi_labels):\n",
    "        avg_signal = np.mean([\n",
    "            signals_by_channel[0][point_idx],\n",
    "            signals_by_channel[1][point_idx],\n",
    "            signals_by_channel[2][point_idx]\n",
    "        ], axis=0)\n",
    "        plt.plot(avg_signal, label=roi_name)\n",
    "\n",
    "    plt.title(\"Filtered 9 Points Graph (BGR Averaged)\")\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(folder_name, \"Filtered_9_Points_Graph.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e2a165",
   "metadata": {},
   "source": [
    "# Start from Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a10e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"input.mp4\"\n",
    "bgr_frames = [] # BGR frames\n",
    "frames = [] # frames for processing\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "start_frame = 69\n",
    "mid_frame = 200 - start_frame\n",
    "last_frame = mid_frame + 200\n",
    "\n",
    "# video sampling rate\n",
    "fs = cap.get(cv2.CAP_PROP_FPS)\n",
    "# print(f\"Video sampling rate: {fs} FPS\")\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Skip first 69 frames\n",
    "for _ in range(start_frame - 1):\n",
    "    ret, _ = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Video too short, cannot reach frame 70.\")\n",
    "\n",
    "# Read the 70th frame\n",
    "ret, frame70 = cap.read()\n",
    "if not ret:\n",
    "    raise ValueError(\"Cannot read the 70th frame from the video.\")\n",
    "\n",
    "# Show frame and let user select ROI\n",
    "roi = cv2.selectROI(\"Select ROI\", frame70, showCrosshair=True, fromCenter=False)\n",
    "cv2.destroyWindow(\"Select ROI\")\n",
    "\n",
    "# roi returns (x, y, w, h), convert to (x1, y1) and (x2, y2)\n",
    "x, y, w, h = roi\n",
    "x1, y1 = x, y\n",
    "x2, y2 = x + w, y + h\n",
    "\n",
    "print(f\"Selected ROI: (x1, y1)=({x1}, {y1}), (x2, y2)=({x2}, {y2})\")\n",
    "\n",
    "upscale = 0 # change it to 1, when we want upscaling. \n",
    "\n",
    "idx = 0\n",
    "\n",
    "# Skip first 69 frames\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame - 1)\n",
    "\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # to observe: run the line in the console: plt.imshow(frame)\n",
    "    # if observed proceed: y1:y2, x1:x2 see the observation after video path\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # w and h, we will look into this later!! we need to do upsampling\n",
    "    # for baseline: no upscaline\n",
    "    \n",
    "    frame =  frame[ y1:y2, x1:x2, :]\n",
    "\n",
    "    if upscale: \n",
    "        w = 600\n",
    "        h = 600\n",
    "        frame =  cv2.resize(frame, (w, h))\n",
    "\n",
    "        \n",
    "    frame = np.float32(frame/255)\n",
    "    bgr_frames.append(frame) # H, W, C\n",
    "    idx += 1\n",
    "    if idx == 2000: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "del cap\n",
    "\n",
    "# List to Numpy Array to Tensor\n",
    "# From: N, H, W, C = Batch Dimensions, Height, Width, Channels\n",
    "# To: N, C, H, W = Batch Dimensions, Channels, Height, Width\n",
    "bgr_frames = torch.from_numpy(np.transpose(bgr_frames, axes=(0, 3, 1, 2))) \n",
    "print(f\"Number of frames: {bgr_frames.shape[0]} or {idx}\")\n",
    "print(f\"Shape of each frame: {bgr_frames.shape[1:]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_, clicked_points = plot_cos_or_dot(bgr_frames[::time_s], clicked_points = None)\n",
    "\n",
    "time_series_data[(time_s, space_s)] =  lines_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ea864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Signal Extraction  and Filtering\n",
    "\n",
    "time_series_BR  = {}\n",
    "\n",
    "for i in time_series_data.keys():\n",
    "    # lines_ = time_series_data[i]\n",
    "    lines_ = list(np.array(time_series_data[i]))\n",
    "    for chan in range(3):\n",
    "        sig_R, sig_R_pos =filt_sig(lines_, 0.05, 0.7, chan, fs= fs/i[0])\n",
    "        time_series_BR[(chan, i)] =  sig_R\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88677ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filtered_graphs(time_series_BR, \"6m_h\", i=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [(start_frame, mid_frame), (mid_frame, last_frame), (last_frame, idx)]\n",
    "channel_names = {0: \"B\", 1: \"G\", 2: \"R\"}\n",
    "points_labels = [\n",
    "    \"Abdomen 1\", \"Abdomen 2\", \"Chest 1\", \"Chest 2\",\n",
    "    \"Shoulder 1\", \"Shoulder 2\", \"Control 1\", \"Control 2\", \"Control 3\"\n",
    "]\n",
    "\n",
    "# print(\"Channel\\tFrame From\\tFrame To\\t\" + \"\\t\".join(points_labels))\n",
    "\n",
    "results = []\n",
    "\n",
    "for chan in [0, 1, 2]:  # B, G, R\n",
    "    key = (chan, (1, 2))\n",
    "    signals = time_series_BR[key]\n",
    "\n",
    "    for (start, end) in ranges:\n",
    "        means = [abs(np.mean(sig[start:end])) for sig in signals]  # <-- abs()\n",
    "        means_str = \"\\t\".join(f\"{m:.6f}\" for m in means)\n",
    "        print(f\"{channel_names[chan]}\\t{start}\\t{end}\\t{means_str}\")\n",
    "        results.append([channel_names[chan], start, end] + means)\n",
    "\n",
    "    # Full range\n",
    "    means_full = [abs(np.mean(sig[0:idx])) for sig in signals]  # <-- abs()\n",
    "    means_full_str = \"\\t\".join(f\"{m:.6f}\" for m in means_full)\n",
    "    print(f\"{channel_names[chan]}\\t0\\t{idx}\\t{means_full_str}\")\n",
    "    results.append([channel_names[chan], 0, idx] + means_full)\n",
    "\n",
    "# --- BGR combined ---\n",
    "for (start, end) in ranges:\n",
    "    combined_means = []\n",
    "    for i in range(9):\n",
    "        all_channels = [time_series_BR[(c, (1, 2))][i][start:end] for c in [0, 1, 2]]\n",
    "        avg_val = np.mean([np.mean(sig) for sig in all_channels])\n",
    "        combined_means.append(abs(avg_val))  # <-- abs()\n",
    "    combined_means_str = \"\\t\".join(f\"{m:.6f}\" for m in combined_means)\n",
    "    print(f\"BGR\\t{start}\\t{end}\\t{combined_means_str}\")\n",
    "\n",
    "combined_full = []\n",
    "for i in range(9):\n",
    "    all_channels = [time_series_BR[(c, (1, 2))][i][0:idx] for c in [0, 1, 2]]\n",
    "    avg_val = np.mean([np.mean(sig) for sig in all_channels])\n",
    "    combined_full.append(abs(avg_val))  # <-- abs()\n",
    "combined_full_str = \"\\t\".join(f\"{m:.6f}\" for m in combined_full)\n",
    "print(f\"BGR\\t0\\t{idx}\\t{combined_full_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a21a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [(start_frame, mid_frame), (mid_frame, last_frame), (last_frame, idx)]\n",
    "roi_labels = [\n",
    "    \"Abdomen 1\", \"Abdomen 2\", \"Chest 1\", \"Chest 2\",\n",
    "    \"Shoulder 1\", \"Shoulder 2\", \"Control 1\", \"Control 2\", \"Control 3\"\n",
    "]\n",
    "\n",
    "# --- Step 1: Compute magnitude BGR averages ---\n",
    "bgr_averages = {}\n",
    "for start, end in ranges:\n",
    "    combined_means = []\n",
    "    for i in range(9):  # 9 ROIs\n",
    "        # Compute mean of each channel, then average them\n",
    "        channel_means = [np.mean(time_series_BR[(c, (1, 2))][i][start:end]) for c in [0, 1, 2]]\n",
    "        avg_val = np.mean(channel_means)\n",
    "        combined_means.append(abs(avg_val))  # <-- take absolute value\n",
    "    bgr_averages[(start, end)] = combined_means\n",
    "\n",
    "# --- Step 2: Compute ratios (ROI magnitude / Control magnitude) ---\n",
    "control_indices = [6, 7, 8]  # Control 1, 2, 3\n",
    "roi_indices = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "print(\"ROI\\tRange\\tControl 1\\tControl 2\\tControl 3\")\n",
    "for roi_idx in roi_indices:\n",
    "    for (start, end) in ranges:\n",
    "        ratios = []\n",
    "        roi_val = bgr_averages[(start, end)][roi_idx]\n",
    "        for c_idx in control_indices:\n",
    "            ctrl_val = bgr_averages[(start, end)][c_idx]\n",
    "            ratio = roi_val / ctrl_val if ctrl_val != 0 else np.nan\n",
    "            ratios.append(ratio)\n",
    "\n",
    "        ratios_str = \"\\t\".join(f\"{r:.4f}\" for r in ratios)\n",
    "        print(f\"{roi_labels[roi_idx]}\\t{start}-{end}\\t{ratios_str}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
